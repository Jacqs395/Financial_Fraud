{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REPORT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     i. Which insights did you gain from your EDA?\n",
    "The initial EDA showed that the dataset was highly imbalanced. There were significantly more non-fraudulent transactions than fraudulent ones. This surprised me because, given the description of the project, I automatically expected to see a high rate of fraud in the data. Even more surprising was that the percentage of transactions flagged as fraud was so small that it didn't even register on a pie chart. Only two of the five transaction types displayed fraudulent activity: Cash_Out and Transfer. This was also shocking for me -- when I started this project, I expected \"Cash-In\" to have the most fraud. Since fraud was prevalent in transactions where money was leaving an account, my guess is that account holders were victims of identity theft, which resulted in exposure of their bank account information, thus leading to withdrawals and bogus transfers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     ii. How did you determine which columns to drop or keep? If your EDA informed this process, explain which insights you used to determine which columns were not needed. \n",
    "The columns dropped were: step, nameOrig, nameDest and isFlagged Fraud. These columns were not as interesting to me as the others, or they had minimal information that did not seem relevant to what I wanted my models to train on. As I was performing EDA on various columns, I decided that I was not really interested in the \"step\" data. Analyzing \"isFraud\" clearly showed that most fraudulent activities were occuring in two specific transaction types: transfers and cash_out. The timeframe of when the fraud occurred was not important to me. I was also not interested in the name of the origin account, or the name of the destination account. What mattered most was the type of transactions where fraud was prevalent, and also the amount. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     iii. Which hyperparameter tuning strategy did you use? Grid-search or random-search? Why?\n",
    "I did not use a hyperparameter for this project. Quite frankly, I ran out of time and so I resolved to skip this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     iv. How did your model's performance change after discovering optimal hyperparameters?\n",
    "As stated above, specific hyperparameters were not used in this project. However, my guess is that hyperparameters would have improved my model's performance signficantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     v. What was your final F1 Score?\n",
    "**LOGISTIC REGRESSION:** the final F1 Score was 0.0435 for fraud. The precision score was low as well (0.02), but on the positive side, there was strong recall (0.96) for fraudulent transactions. \n",
    "\n",
    "**NAIVE BAYES:** the final F1 Score was an abysmal 0.004. Clearly, this model performed very poorly. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
